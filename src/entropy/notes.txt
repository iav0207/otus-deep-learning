
В формуле P{y,x} логистической регрессии y обычно 0 или 1 (но не всегда), соответственно мы можем предсказывать
наступление или ненаступление события, в зависимости от этого избавляемся от одного из двух множителей.

Допустим, мы хотим научиться предсказывать дождь в зависимости от некоторого вектора значений x:
- температура в точке А в данный момент
- т в т B в данный момент
- т в т А за день до этого
- [влажности]
- [курсы валют]
- что угодно

для обучения нам потребуется исторический сет из векторов x и известные исходы (дождь наступил или нет, или ну так).
мы подгоним параметры сигмоид (теты слоёв) при помощи регрессии, и на зафиксированных тетах будем обрабатывать
боевые датасеты из векторов x.

Функция правдоподобия (вероятность корректности модели)

Произведение вероятностей того, что наша модель предсказала правильный результат, для каждого возможного исхода
на конкретном датасете.

То есть обучив модель на тренировочном сете, можно затем измерять насколько хорошо она работает
на тестовых (тоже с известными исходами).

Поскольку правдоподобие – это вероятность, то отрицательный логарифм от неё даёт количество информации,
которую мы получаем, проверяя корректность модели.
ВНЕЗАПНО это и есть перекрёстная (cross-) энтропия между нашими предсказаниями и реальными данными.

Относительная энтропия

Если Q – нейросеть, а P – реальная система, то минимизация их кросс-энтропии
и есть минимизация дивергенции Кульбака-Лейблера

Softmax

Первый трюк хорош тем, что если у нас вектор [0, 100], то мы не влетим по памяти пытаясь вычислить e^100,
вместо этого мы вычнем максимальный элемент из вектора и вычислим e^(-99) и e^(0). Гарантированное наличие
слагаемого e^0 хорошо сказывается также на знаменателе, который благодаря этому не менее единицы (то есть достаточно
велик, чтобы сохранялась точность числа с плавающей точкой).

Второй трюк также позволяет избежать возведения e в высокую степень.


