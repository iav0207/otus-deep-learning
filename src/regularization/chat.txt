09:57:10	 From Pavel Bedyaev : Видно\слышно
09:57:26	 From Pavel Bedyaev : да
09:57:28	 From Nikita Dovidchenko  To  Otus Онлайн-образование(privately) : +
09:57:33	 From pavelkrylov : +
10:03:06	 From Василий Володин : -
10:03:08	 From Nikita Dovidchenko : -
10:03:09	 From Pavel Bedyaev : +
10:03:09	 From pavelkrylov : +
10:03:10	 From Valeriya I : +
10:03:13	 From Artem : -
10:03:14	 From andrey ryzhkov : -
10:03:16	 From alx : +-
10:07:41	 From Михаил Михайлович : перемешать
10:07:42	 From andrey ryzhkov : -
10:07:44	 From alx : Как повезет
10:07:46	 From Nikita Dovidchenko : если все так связано с погодой то нет
10:07:57	 From pavelkrylov : _
10:12:49	 From alx : -
10:12:53	 From Nikita Dovidchenko : -
10:14:23	 From atercygnus : leave_one_out
10:14:38	 From andrey ryzhkov : несколько раз по-разному переразбивать?
10:14:39	 From Pavel Bedyaev : это что?
10:14:39	 From Михаил Михайлович : а кросс валидация
10:21:01	 From Nikita Dovidchenko : -
10:21:01	 From alx : -
10:21:03	 From Василий Володин : -
10:21:03	 From Valeriya I : -
10:21:05	 From sergo : -
10:21:09	 From andrey ryzhkov : -
10:21:13	 From atercygnus : Думаю, надо сказать, что переобучаемся мы, когда мало данных..
10:21:23	 From atercygnus : ...для такого когличества параметров...
10:22:10	 From atercygnus : берт?
10:22:13	 From atercygnus : GPT2
10:25:19	 From atercygnus : валидационная выборка нужна
10:25:22	 From alx : Еще раз вопрос можно
10:25:46	 From Василий Володин : не очень понятно
10:26:20	 From Василий Володин : что такое глубина?
10:26:51	 From Pavel Bedyaev : давайте будем считать что ни у кого нет бэкграунда
10:27:18	 From Михаил Михайлович : отложенная выборка?
10:28:41	 From Pavel Bedyaev : т.е. на тесте только смотрим качество, но не подстраиваемся
10:29:30	 From atercygnus : ...а если мы ставим несколько разных серий разных экспериментов, можем ли мы переобучиться ещё и на валидационную, и нужно ли ещё немного откусить от треина?
10:30:04	 From Василий Володин : на валидационном мы тоже обучаемся?
10:30:57	 From Михаил Михайлович : если завалили отложенную, то все с начало?
10:31:50	 From Михаил Михайлович : тестовую
10:32:08	 From Василий Володин : чем валидационная отличается от тестовой, они формально похожи
10:34:57	 From Михаил Михайлович : можно про кросс валидацию временных рядов?
10:35:22	 From Михаил Михайлович : там же просто так разбить нелтзя
10:35:26	 From Василий Володин : получается что валидация нужна для быстрой проверки типов нейросетей на данной выборке?
10:35:51	 From Михаил Михайлович : в целом,  много работаю с времеными рядами
10:36:57	 From atercygnus : Расскажите про кросс-валидацию во временных рядах, и вообше в любых time-sensitive данных...
10:38:00	 From aldar : https://habr.com/ru/company/ods/blog/327242/
10:38:03	 From aldar : Тут хорошо показано
10:38:36	 From Nikita Dovidchenko : +
10:38:41	 From pavelkrylov : +
10:38:43	 From atercygnus : +
10:38:44	 From Pavel Bedyaev : +
10:38:52	 From Михаил Михайлович : +
10:39:14	 From atercygnus : Вот картинка https://www.google.ru/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&ved=2ahUKEwjX49SS0bTmAhUNwqYKHR8LCdQQjRx6BAgBEAQ&url=http%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fcross_validation.html&psig=AOvVaw2uvrHc5d0Pd8qdfyoLO_mQ&ust=1576395538412043
10:39:33	 From atercygnus : https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png
10:39:37	 From atercygnus : ой, вот
10:43:11	 From Nikita Dovidchenko : +
10:43:15	 From andrey ryzhkov : +
10:43:18	 From aldar : +
10:43:20	 From Михаил Михайлович : в боевом режиме разве не радо на всем датасете учить?
10:43:36	 From Михаил Михайлович : когда в продакшен
10:43:56	 From alx : А как точно данный метод остановки называется?
10:43:59	 From Михаил Михайлович : если на всем, то как узнать что останавливаться
10:44:01	 From alx : curve stopping?
10:44:21	 From Otus Онлайн-образование  To  Nikita Dovidchenko(privately) : Early Stopping
10:44:39	 From aldar : Early stopping
10:44:40	 From Pavel Bedyaev : что такое эпоха?
10:44:42	 From Otus Онлайн-образование : Early Stopping
10:45:47	 From Pavel Bedyaev : как формируется батч?
10:45:58	 From Pavel Bedyaev : да)
10:48:07	 From Pavel Bedyaev : нужна ли стратификация? или рандома достаточно?
10:48:53	 From atercygnus : я так понимаю, одна крайность - когда мы ошибку считаем на всем обучающем множестве на каждом шаге обучения, другая - когда ошибка считается на одном объекте(SGD). А батчи - это что-то среднее...
10:51:29	 From Pavel Bedyaev : отл, полезность батчей понятна, спасибо
10:52:24	 From Pavel Bedyaev : батчи по экземплярам обучающей выборки пересекаются?
10:52:50	 From atercygnus : чем больше размер батча - тем стабильнее сходимость, но дольше обучение(как правило), да?
10:53:37	 From Михаил Михайлович : обычно наоборот 
10:54:16	 From Pavel Bedyaev : т.е. на практике размер батча надо делать таким чтобы влезал в память и их было несколько на обучающую выборку (а не один-два), верно?
10:56:05	 From Pavel Bedyaev : если нет ограничений по железу, на какое количество батчей бить выборку?
10:56:08	 From Михаил Михайлович : в видюху только степень двойки влазиет размеры
10:57:28	 From Pavel Bedyaev : понятно) я хотел понять есть ли какие-то пределы, типа один батч это плохо, миллиард тоже плохо а между норм
10:59:53	 From atercygnus : Я читал у Дьяконова, что в случае линейной регрессии не надо свободный член подвергать регуляризации. В стандартных фреймворках это так и делается?
11:01:38	 From atercygnus : А есть ещё параметры, которые не надо подвергать регуляризации?
11:02:02	 From atercygnus : И да, и в других алгоритмах.
11:02:55	 From atercygnus : l1, елси ошибка распределена по Лапламсу, д2
11:03:02	 From atercygnus : l2, если нормально
11:03:12	 From aldar : l1 - уменьшает веса, l2 - обнуляет
11:05:12	 From Pavel Bedyaev : -
11:05:14	 From Василий Володин : -
11:05:14	 From atercygnus : -
11:05:16	 From aldar : +-
11:05:16	 From pavelkrylov : -
11:07:33	 From Pavel Bedyaev : все еще -
11:08:02	 From aldar : Это как-то связано с тем, что L2 - это евклидова дистанция (возведенная в квадрат)
11:11:41	 From atercygnus : Круто, я этого не понимал раньше)
11:12:09	 From Otus Онлайн-образование : The elements of statistical learning
11:13:40	 From atercygnus : ММП теоретически минимизирует правдоподобие из предположения, что ошибка алгоритма распределена нормально в случае l2. А если ошибка Лапласова, то l1. А если ошибка имеет другое распределение?
11:14:49	 From atercygnus : метод макс. правдоподобия
11:15:47	 From atercygnus : Тогда как раз ошибка будет распределена биномиально
11:17:21	 From atercygnus : Так мы же и определяем функцию ошибка как правдоподобие...кросс-энтропия та же....
11:17:39	 From atercygnus : вводили
11:18:32	 From Pavel Bedyaev : вот мне про relu так никто и не ответил. все что нашел - вычислительно проще
11:18:35	 From atercygnus : В книжке Николенко и Кадурина есть теор. объяснене, почему релу работает
11:19:04	 From Pavel Bedyaev : про сигмоид vs relu, почему relu сейчас популярна
11:19:08	 From atercygnus : я найду и скину в слак.
11:19:11	 From pavelkrylov : Может мы пойдем дальше как то практику хочется пройти
11:19:25	 From pavelkrylov : у нас все же лекция не для одного человека
11:23:05	 From Pavel Bedyaev : т.е. задается процент выключаемых? а не порог для каждого?
11:24:08	 From Pavel Bedyaev : выключаемые выбираются рандомно?
11:24:34	 From Василий Володин : выключаем в рамках одного батча или эпохи?
11:27:30	 From atercygnus : Мы добавляем данные, а чем больше данных, тем меньше переобучение.
11:28:35	 From atercygnus : +
11:28:36	 From aldar : +
11:28:37	 From sergo : +
11:28:37	 From andrey ryzhkov : +
11:28:43	 From pavelkrylov : +
11:28:44	 From atercygnus : надо бы руками это сделать, чтобы понять
11:28:47	 From Василий Володин : поделим на каждом слое нужно?
11:28:51	 From Pavel Bedyaev : +
11:29:57	 From atercygnus : да, это про шум
11:30:35	 From aldar : Я где-то читал, что дропаут замедляет обучение, это верно на практике?
11:32:00	 From atercygnus : дз было)
11:32:02	 From pavelkrylov : Да было первая Дамаска на это
11:32:33	 From Василий Володин : как раз зачем не понимаем
11:33:51	 From andrey ryzhkov : * ранняя остановка лишает сеть возможности зазубрить данные, не давая времени.
* дропауты - что-то вроде сеть лишаем памяти, поэтому вместо зубрежки приходится обобщать. 
* а по регуляризации есть аналогия или образ, или только формула l1 2?
11:35:39	 From Pavel Bedyaev : Как все-таки правильно нормализовать данные? Весь датасет, или каждый батч отдельно?
11:38:31	 From andrey ryzhkov : спасибо
11:41:45	 From sergo : а батч нормализация происходит между слоями или на вход?
11:42:20	 From sergo : аошибка распрстранения ак назад проходит?
11:42:21	 From Василий Володин : от батча к батчу эта яркость тоже будет плавать если будут в целом более яркие или более тусклые картинки
11:43:18	 From atercygnus : гамма и бета настраиваются градиентным спуском?
11:44:41	 From atercygnus : Так в нейросетях много у чего нет строгого обоснования, там куча эвристик)
11:47:36	 From atercygnus : А какими оказываются гамма и бета на практике после настройки градиентым спуском? Далеки от 1 и 0 соответственно?
11:49:09	 From Pavel Bedyaev : Есть, но пока не могу сформулировать. Переспрошу попозже.
11:50:52	 From sergo : наверно нет
11:50:56	 From atercygnus : Почему нет?
11:51:17	 From Pavel Bedyaev : м.б. распределение меняется после дропаута?
11:51:18	 From sergo : нмалиация при отключке 
11:51:25	 From atercygnus : Или гамма и бета бубт настраиваться на нули, которые окажутся после дпроаута?
11:54:42	 From Pavel Bedyaev : а можем комбинировать но с разносом по эпохам?
11:58:40	 From atercygnus : -
11:58:41	 From andrey ryzhkov : -
11:58:42	 From Pavel Bedyaev : -
11:59:42	 From andrey ryzhkov : +
12:01:47	 From pavelkrylov : Else там отполз
12:02:02	 From pavelkrylov : 10 ячейка
12:02:22	 From atercygnus : просто таб поставьте
12:03:28	 From atercygnus : а, это елс от фора, а не от ифа.
12:03:32	 From Otus Онлайн-образование : https://stackoverflow.com/questions/9979970/why-does-python-use-else-after-for-and-while-loops
12:03:46	 From pavelkrylov : ок отл
12:04:00	 From Pavel Bedyaev : неочевидно, спасибо
12:04:18	 From alx : for-else -- это очень удобно
12:05:39	 From Pavel Bedyaev : как тут считается error ?
12:05:51	 From Pavel Bedyaev : а, ок
12:07:04	 From atercygnus : Можно сделать вывод, что дроп не нужен вообще.
12:07:06	 From Pavel Bedyaev : значит по росту кросс энтропии можем детектировать наличие переобучения?
12:09:39	 From sergo : перееобучение
12:09:54	 From Pavel Bedyaev : подбирается более правильная функция
12:09:55	 From atercygnus : Мы ошибаемся не там, где истинный ответ
12:10:35	 From atercygnus : ...или сеть становится менее уверенной на истинном значении, но оно всё ещё максимальное.
12:10:45	 From andrey ryzhkov : тестовые ответы все хуже, из тренинг сета все точнее
12:11:36	 From Pavel Bedyaev : тут же только тест, без трейна?
12:14:53	 From atercygnus : -
12:14:55	 From andrey ryzhkov : -
12:14:59	 From pavelkrylov : -
12:15:04	 From Pavel Bedyaev : можно еще раз именно на этих графиках, линию где наступило переобчуение?
12:15:21	 From Otus Онлайн-образование : https://otus.ru/polls/6241/
12:16:57	 From Pavel Bedyaev : да, спасибо
12:17:39	 From Nikita Dovidchenko : спасибо!
12:17:53	 From Pavel Bedyaev : Спасибо! Было очень интересно
12:17:56	 From pavelkrylov : Спасибо!
12:18:06	 From andrey ryzhkov : спасибо!
