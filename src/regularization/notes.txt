
регуляризация – вообще непонятно что и почему

dropout – последовательное искусственное выключение случайных нейронов в ходе обучения, чтобы избежать "запоминание"
отдельными нейронами специфичных паттернов и "индивидуальное переобучение" на них. Кроме того, это делает сеть
более устойчивой, т.к. нейроны вынуждены не опираться слишком сильно на показания какого-то одного (или немногих)
"важного" нейрона из предыдущего слоя.
Технически dropout реализуется добавлением промежуточных слоёв с тем чтобы обнулить функции активации отдельных нейронов.

dropout есть по сути выделение подсетей из сети и обучение их отдельно _на разных батчах_ (не по целым эпохам)

ещё один метод повышения устойчивости сети – добавление гауссова шума к слоям

батч-нормализация – нормирование активации слоя используя статистики текущего батча. Нейросети сложно обучаться,
когда на вход идут батчи с отличающимися распределениями. Говоря простыми словами, этот метод помогает избежать
ситуации, когда нейросеть распознаёт символ на картинке, опираясь на такие параметры, как яркость или контраст,
просто потому что в обучающих выборках был сдвиг на этом символе.

dropout замедляет обучение, batch-norm – ускоряет
dropout + batch-norm не сочетаются. dropout портит распределения между слоями, нет смысла нормализовывать батч

сеть переобучается, когда увеличение числа эпох начинает увеличивать ошибку (loss), это время когда надо остановиться

loss – это кросс-энтропия

loss и accuracy не совсем обратные понятия, в примере в ноутбуке можно видеть, что в течение некоторых эпох
обе величины возрастают. сеть более уверена в трудноразличимых пограничных случаях,
но допускает всё более грубые систематические ошибки

